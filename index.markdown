---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: single
title: ""
author_profile: true
---
## About Me
Howdy! My name is Yufeng Yang (æ¨é’°å³° ,pronounced as u-feng iang). I'm a third year PhD student at CSE department, **Texas A&M University**, advised by [Prof. Yi Zhou](https://sites.google.com/site/yizhouhomepage/home). I leverage **foundational ML principles** to address the computational and algorithmic challenges arising from **large-scale machine learning**. My current and prospective research focuses on:

> **Distributionally Robust Optimization for ML**: Since the beginning of my Ph.D., my research has been driven by the challenges of distribution shifts in large-scale datasets. Specifically, I focus on ML training paradigms subject to distributional drift, utilizing optimization theory to convert loss formulations under uncertainty into tractable, solvable problems. I am particularly interested in bridging this framework to training scenarios such as **adversarial training, multi-task learning, Reinforcement Learning and LLM pre/post-training**.

> **Efficient first/zeroth-order Optimization Algorithm**: Modern modeling techniquesâ€”including sequential modeling (RNN and SSM variants), multi-modal learning, and on-policy RL often result in ill-conditioned geometric loss landscapes and heavy-tailed representation distributions. My research focuses on designing efficient **first and zeroth-order optimization** algorithms for these complex formulations, including **bi-level, compositional, and contextual dependent objectives**. I am dedicated to exploring relationship between *problem-dependent parameters, algorithmic structures (such as **learning rate scheduling, normalization/clipping, variance reduction, momentum/extrapolation, pre-conditioning etc**) and the resulting convergence performance*.

Furthermore, I am interested in emerging topics such as system-aware optimization and RL algorithms tailored for large-scale GPU clusters. **My goal is to bridge foundational theory with AI infrastructure to maximize system-level performance**, alongside exploring the mechanistic interpretability of LLMs.

<span style="color:red">**(Open for Collaboration)**</span> If you believe our research interests align and want to collaborate with me. Feel free to drop me an email at [ynyang94@tamu.edu](mailto:nyyang94@tamu.edu) or add me on WeChat: **ynyang94** (please indicate your purpose when connecting).

<span style="color:red;"> **Iâ€™m actively looking for AI research scientist/engineer position internship starting at 26/27 summer**. Here is my brief [CV](https://ynyang94.github.io/my_presentation/YufengYang_short_CV.pdf).</span>


## News
<div style="height: 100px; overflow-y: auto; border: 1px solid #ddd; padding: 10px;">
<li><strong>March 2026:</strong> Attend IOS at Atlanta,GA and give a talk on Sinkhorn DRO.</li>
<li><strong>Sep 2025:</strong> Attend 2025 Mathematical and Scientific Foundations of Deep Learning Annual Meeting.</li>
<li><strong>July 2025:</strong> Attend ICCOPT 2025, USC, Los Angeles and present our work IAN-SGD.</li>
<li><strong>June 2025:</strong> Got my first long paper accepted by TMLR. Special thank for my collaborators and AFRL for their supervision and funding support during summer 2024. </li>
<li><strong>March 2025:</strong> Pass fundamental exam marked as a "formal" starting point of my PhD journey.</li>
<li><strong>Dec 2024:</strong> Attend NeurIPS 2024, Vancouver, CA and present our work on Sinkhorn DRO.</li>
<li><strong>Sep 2023:</strong> Attend Allerton Conference host at Allerton Park by UIUC.</li>
</div>


## ðŸ“„Papers
*Nested SGD for Sinkhorn distance-regularized Distributionally Robust Optimization*\[[arXiv](https://arxiv.org/abs/2503.22923)\], \[[code](https://github.com/ynyang94/GeneralSinkhorn-Regularized-DRO)\],Submitted; \[[short version](https://openreview.net/pdf?id=qdxx8cqu80)\] accepted at OPT workshop, Neurips 2024 \[[poster](https://ynyang94.github.io/my_presentation/NIPS_workshop_poster-2.pdf)\].

*Adaptive Gradient Normalization and Independent Sampling for (Stochastic) Generalized-Smooth Optimization*\[[arxiv](https://arxiv.org/abs/2410.14054)\], \[[code](https://github.com/ynyang94/Gensmooth-IAN-SGD)\], \[[slides](https://ynyang94.github.io/my_presentation/ICCOPT2025_IAN_SGD-6.pdf)\], TMLR.

## ðŸ“šEducation
-- **Ph.D.** in Computer Science, Texas A&M University, 2024-now

-- **Ph.D.**(Transfer Out) in Electrical Engineering, University of Utah, 2023-2024

-- **M.S.** in Computational Science and Engineering, Rice University, 2021-2023

-- **B.S.** in Applied Math (SSE), Chinese University of Hong Kong(Shenzhen), 2017-2021

Prior to university, I grew up and finished my elementary education in Jiayuguan, a small town in Gansu Province, China, located near the western starting point of the Great Wall.


## Academic Services
-- Conference: AISTATS\
-- Journal Reviewer: IEEE TSP; IEEE TPAMI; Journal of Combinatorial Optimization;\
-- Workshop Reviewer: NeurIPS-OPT workshop

## Teaching
-- At Rice: ELEC241, Fundamentals of Electrical Engineering I (Role: Grader).

<!-- ClustrMaps Tracker -->
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Mn46VZvuelQyX06Ut7_UBUIgSG5O9ztIhNIRwUwdmhU&cl=ffffff&w=a"></script>
