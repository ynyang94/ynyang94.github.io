---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: single
title: ""
author_profile: true
---
## About Me
Howdy! My name is Yufeng Yang (æ¨é’°å³° ,pronounced as u-feng iang). I'm a third year PhD student at CSE department, **Texas A&M University**, advised by [Prof. Yi Zhou](https://sites.google.com/site/yizhouhomepage/home). My general research interests revolve around the **foundations of machine learning**.  Specially, I'm interested in the following fields:

> **Stochastic Optimization**: I have been working on this topic since the beginning of my Ph.D. program. My research focuses on designing stochastic algorithms (typically first-order or zeroth-order methods) to solve tractable formulations inspired by large-scale machine learning, such as distributionally robust optimization (DRO), multi-objective optimization, and stochastic programming with heavy-tailed noise etc.

> **Mathematics of Deep Learning**: In the era of large language models (LLMs), I am particularly interested in the mathematical insights behind preconditioned optimization methodsâ€”such as Muon(and shampoo) and Adam (and its variants)â€”as well as heuristics like learning rate scheduling and warm-up strategies. I am also passionate about exploring the mathematical foundations and mechanical interpretations of deep networks, especially for transformer-based LLMs, including phenomena such as transformer training dynamics, LLM grokking, in-context learning, and scaling laws.

> **Reinforcement Learning Theory and Post-Training of Agentic LLM** I am interested in reinforcement learning (RL) post-training/alignment methods, especially evolving on-policy methods such as PPO, GRPO etc. I'm also intersted in RL-theory intersecting with stochastic optimization, including distributional RL, Multi-objective/agentic RL methods.

If you believe our research interests align, or want to know more about me, our research group, department and lives in College Station. Feel free to drop me an email at [ynyang94@tamu.edu](mailto:nyyang94@tamu.edu) or add me on WeChat: **ynyang94** (please indicate your purpose when connecting).

<span style="color:red;">I'm actively looking for machine learning engineer, data science and quantative internship opportunities during 2026 summer (Due to visa policy, I have to limit job positions in USA, mainland China/Hong Kong SAR). Here is my breif \[(CV)(https://ynyang94.github.io/my_presentation/YufengYang_short_CV-3.pdf)\].  </span>


## News
<div style="height: 100px; overflow-y: auto; border: 1px solid #ddd; padding: 10px;">
<li><strong>Sep 2025:</strong> Attend Simons Workshop on Deep Learning Theory.</li>
<li><strong>July 2025:</strong> Attend ICCOPT 2025, USC, Los Angeles and present our work IAN-SGD.</li>
<li><strong>June 2025:</strong> Got my first long paper accepted by TMLR. Special thank for my collaborators and AFRL for their supervision and funding support during summer 2024. </li>
<li><strong>March 2025:</strong> Pass fundamental exam marked as a "formal" starting point of my PhD journey.</li>
<li><strong>Dec 2024:</strong> Attend NeurIPS 2024, Vancouver, CA and present our work on Sinkhorn DRO.</li>
<li><strong>Sep 2023:</strong> Attend Allerton Conference host at Allerton Park by UIUC.</li>
</div>


## ðŸ“„Papers
*Nested SGD for Sinkhorn distance-regularized Distributionally Robust Optimization*\[[arXiv](https://arxiv.org/abs/2503.22923)\], \[[code](https://github.com/ynyang94/GeneralSinkhorn-Regularized-DRO)\],Submitted; \[[short version](https://openreview.net/pdf?id=qdxx8cqu80)\] accepted at OPT workshop, Neurips 2024 \[[poster](https://ynyang94.github.io/my_presentation/NIPS_workshop_poster-2.pdf)\].

*Adaptive Gradient Normalization and Independent Sampling for (Stochastic) Generalized-Smooth Optimization*\[[arxiv](https://arxiv.org/abs/2410.14054)\], \[[code](https://github.com/ynyang94/Gensmooth-IAN-SGD)\], \[[slides](https://ynyang94.github.io/my_presentation/ICCOPT2025_IAN_SGD-6.pdf)\], TMLR.

## ðŸ“šEducation
-- **Ph.D.** in Computer Science, Texas A&M University, 2024-now

-- **Ph.D.**(Transfer Out) in Electrical Engineering, University of Utah, 2023-2024

-- **M.S.** in Computational Science and Engineering, Rice University, 2021-2023

-- **B.S.** in Applied Math (SSE), Chinese University of Hong Kong(Shenzhen), 2017-2021

Prior to university, I grew up and finished my elementary education in Jiayuguan, a small town in Gansu Province, China, located near the western starting point of the Great Wall.


## Academic Services
-- Conference: AISTATS\
-- Journal Reviewer: Journal of Combinatorial Optimization; IEEE Transactions on Signal Processing\
-- Workshop Reviewer: NeurIPS-OPT workshop

## Teaching
-- At Rice: ELEC241, Fundamentals of Electrical Engineering I (Role: Grader).

<!-- ClustrMaps Tracker -->
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Mn46VZvuelQyX06Ut7_UBUIgSG5O9ztIhNIRwUwdmhU&cl=ffffff&w=a"></script>
