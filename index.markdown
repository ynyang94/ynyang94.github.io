---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: single
title: ""
author_profile: true
---
## About Me
Howdy! My name is Yufeng Yang (杨钰峰 ,pronounced as u-feng iang). I'm a Ph.D. student at CSE department, **Texas A&M University**, advised by [Prof. Yi Zhou](https://sites.google.com/site/yizhouhomepage/home). My general research interests revolve around the **foundations of machine learning**.  Specially, I'm interested in the following fields:

> **Stochastic Optimization**: I have been working on this topic since the beginning of my Ph.D. program. My research focuses on designing stochastic algorithms (typically first-order or zeroth-order methods) to solve tractable formulations inspired by large-scale machine learning, such as distributionally robust optimization (DRO), multi-objective optimization, and stochastic programming with heavy-tailed noise etc.

> **Mathematics of Deep Learning**: In the era of large language models (LLMs), I am particularly interested in the mathematical heuristics underlying preconditioned optimization methods—such as Muon, Shampoo, and AdamW—as well as techniques like learning rate scheduling and warm-up strategies. I am also passionate about exploring the mathematical foundations and mechanical interpretations of deep networks, especially LLMs, including phenomena such as grokking, catastrophic forgetting, in-context learning, and scaling laws.

> **Reinforcement Learning Theory and post-training of LLM** I am interested in reinforcement learning (RL) post-training methods, as well as theoretical aspects of RL, including distributional robust RL, multi-agent RL, and multi-objective RL.

If you believe our research interests align, feel free to reach out to me for potential collaboration at [ynyang94@tamu.edu](mailto:nyyang94@tamu.edu) or add me on WeChat: **ynyang94** (please indicate your purpose when connecting).


## News
<div style="height: 20px; overflow-y: auto; border: 1px solid #ddd; padding: 10px;">
</div>

## 📄Papers
*Nested SGD for Sinkhorn distance-regularized Distributionally Robust Optimization*\[[arXiv](https://arxiv.org/abs/2503.22923)\]\[[code](https://github.com/ynyang94/GeneralSinkhorn-Regularized-DRO)\],Submitted; \[[short version](https://openreview.net/pdf?id=qdxx8cqu80)\] accepted at OPT workshop, Neurips 2024.

*Adaptive Gradient Normalization and Independent Sampling for (Stochastic) Generalized-Smooth Optimization*\[[arxiv](https://arxiv.org/abs/2410.14054)\] \[[code](https://github.com/ynyang94/Gensmooth-IAN-SGD)\], to appear at TMLR.

## 📚Education
-- **Ph.D.** in Computer Science, Texas A&M University, 2024-now

-- **Ph.D.**(Transfer Out) in Electrical and Computer Engineering, University of Utah, 2023-2024

-- **M.S.** in Computational Science and Engineering, Rice University, 2021-2023

-- **B.S.** in Applied Math (SSE), Chinese University of Hong Kong(Shenzhen), 2017-2021

Prior to university, I grew up and finished my elementary education in Jiayuguan, a small town in Gansu Province, China, located near the western starting point of the Great Wall.

## Academic Services
-- Conference/Workshop Reviewer: AISTATS(2024), NeurIPS 2025 OPT workshop
-- Journal Reviewer: Journal of Combinatorial Optimization

## Teaching
-- At Rice: ELEC241, Fundamentals of Electrical Engineering I (Role: Grader).

<!-- ClustrMaps Tracker -->
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Mn46VZvuelQyX06Ut7_UBUIgSG5O9ztIhNIRwUwdmhU&cl=ffffff&w=a"></script>
